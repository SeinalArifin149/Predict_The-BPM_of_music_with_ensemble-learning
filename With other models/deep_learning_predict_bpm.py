# -*- coding: utf-8 -*-
"""Deep-Learning_Predict-BPM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aGzV9gxuygvuQ8MOfj-4XSDOvPAm2jZb

# Import Dataset from kaggle
"""

# import kagglehub
# kagglehub.login()
# # 7f16fd196cb0b051b19dff2f9127fb7c

# playground_series_s5e9_path = kagglehub.competition_download('playground-series-s5e9')

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# !kaggle competitions download -c playground-series-s5e9
# !unzip playground-series-s5e9.zip

# print("âœ… Data source import complete.")



"""# Anu

"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load data
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")
sample = pd.read_csv("sample_submission.csv")

# Feature engineering function (existing)
def create_features(df):
    df = df.copy()
    df['Energy_Loudness'] = df['Energy'] * df['AudioLoudness']
    df['Rhythm_Energy'] = df['RhythmScore'] * df['Energy']
    df['Rhythm_Loudness'] = df['RhythmScore'] * df['AudioLoudness']
    df['Energy_to_Rhythm'] = df['Energy'] / (df['RhythmScore'] + 1)
    df['Loudness_to_Energy'] = df['AudioLoudness'] / (df['Energy'] + 1)
    df['Rhythm_to_Loudness'] = df['RhythmScore'] / (abs(df['AudioLoudness']) + 1)
    
    numerical_cols = ['RhythmScore', 'Energy', 'AudioLoudness']
    df['Mean_Features'] = df[numerical_cols].mean(axis=1)
    df['Std_Features'] = df[numerical_cols].std(axis=1)
    df['Max_Feature'] = df[numerical_cols].max(axis=1)
    df['Min_Feature'] = df[numerical_cols].min(axis=1)
    
    df['Energy_Squared'] = df['Energy'] ** 2
    df['Rhythm_Squared'] = df['RhythmScore'] ** 2
    df['Energy_Root'] = np.sqrt(abs(df['Energy']))
    df['Rhythm_Root'] = np.sqrt(abs(df['RhythmScore']))
    
    return df

# Prepare data
y = train["BeatsPerMinute"]
x = create_features(train.drop(["id", "BeatsPerMinute", "VocalContent", "TrackDurationMs", "LivePerformanceLikelihood"], axis=1))
x_test_kaggle = create_features(test.drop(["id", "VocalContent", "LivePerformanceLikelihood", "TrackDurationMs"], axis=1))
test_ids = test["id"]

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(x)
x_test_kaggle_scaled = scaler.transform(x_test_kaggle)

# Split data for training and validation
X_train, X_val, y_train, y_val = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(f"Training set shape: {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")

# Build the model (corrected syntax)
model = models.Sequential([
    layers.Dense(512, activation="relu", input_shape=(X_train.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.2),
    layers.Dense(64, activation="relu"),
    layers.Dense(1)
])

# Compile model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss="mae",
    metrics=["mae", "mse"]
)

model.summary()

# Callbacks for better training
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=10,
    min_lr=1e-6
)

# Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=128,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# ================================
# MODEL EVALUATION & ANALYSIS
# ================================

# Make predictions
train_pred = model.predict(X_train)
val_pred = model.predict(X_val)

# Calculate metrics
train_mae = mean_absolute_error(y_train, train_pred)
val_mae = mean_absolute_error(y_val, val_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
train_r2 = r2_score(y_train, train_pred)
val_r2 = r2_score(y_val, val_pred)

print("\n" + "="*50)
print("MODEL EVALUATION RESULTS")
print("="*50)
print(f"Training MAE: {train_mae:.4f}")
print(f"Validation MAE: {val_mae:.4f}")
print(f"Training RMSE: {train_rmse:.4f}")
print(f"Validation RMSE: {val_rmse:.4f}")
print(f"Training RÂ²: {train_r2:.4f}")
print(f"Validation RÂ²: {val_r2:.4f}")

# Overfitting/Underfitting Analysis
print("\n" + "="*50)
print("OVERFITTING/UNDERFITTING ANALYSIS")
print("="*50)

mae_difference = abs(train_mae - val_mae)
rmse_difference = abs(train_rmse - val_rmse)

if val_mae > train_mae * 1.1 and mae_difference > 2.0:
    print("ğŸ”´ OVERFITTING DETECTED!")
    print("- Validation error significantly higher than training error")
    print("- Consider: more regularization, dropout, or early stopping")
elif train_mae > 10 and val_mae > 10:
    print("ğŸŸ¡ UNDERFITTING DETECTED!")
    print("- Both training and validation errors are high")
    print("- Consider: more complex model, more features, or less regularization")
else:
    print("ğŸŸ¢ MODEL IS WELL-FITTED!")
    print("- Good balance between bias and variance")

print(f"MAE Difference: {mae_difference:.4f}")
print(f"RMSE Difference: {rmse_difference:.4f}")

# ================================
# VISUALIZATION
# ================================

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Training History - Loss
axes[0, 0].plot(history.history['loss'], label='Training Loss', color='blue')
axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', color='red')
axes[0, 0].set_title('Model Loss Over Time')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss (MAE)')
axes[0, 0].legend()
axes[0, 0].grid(True)

# 2. Training History - MAE
axes[0, 1].plot(history.history['mae'], label='Training MAE', color='blue')
axes[0, 1].plot(history.history['val_mae'], label='Validation MAE', color='red')
axes[0, 1].set_title('Model MAE Over Time')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('MAE')
axes[0, 1].legend()
axes[0, 1].grid(True)

# 3. Predictions vs Actual (Validation)
axes[1, 0].scatter(y_val, val_pred.flatten(), alpha=0.5)
axes[1, 0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
axes[1, 0].set_title('Predictions vs Actual (Validation)')
axes[1, 0].set_xlabel('Actual BPM')
axes[1, 0].set_ylabel('Predicted BPM')
axes[1, 0].grid(True)

# 4. Residuals Plot
residuals = y_val - val_pred.flatten()
axes[1, 1].scatter(val_pred.flatten(), residuals, alpha=0.5)
axes[1, 1].axhline(y=0, color='r', linestyle='--')
axes[1, 1].set_title('Residuals Plot (Validation)')
axes[1, 1].set_xlabel('Predicted BPM')
axes[1, 1].set_ylabel('Residuals')
axes[1, 1].grid(True)

plt.tight_layout()
plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
plt.show()

# ================================
# FINAL PREDICTIONS & SUBMISSION
# ================================

# Make predictions on test set
test_predictions = model.predict(x_test_kaggle_scaled)

# Create submission dataframe
submission = pd.DataFrame({
    "id": test_ids,
    "BeatsPerMinute": test_predictions.flatten()
})

# Save submission file
submission.to_csv("submission.csv", index=False)

print("\n" + "="*50)
print("SUBMISSION FILE CREATED")
print("="*50)
print(f"Submission shape: {submission.shape}")
print(f"Prediction range: {test_predictions.min():.2f} - {test_predictions.max():.2f}")
print("âœ… File 'submission.csv' saved successfully!")

# Display first few predictions
print("\nFirst 10 predictions:")
print(submission.head(10))

# ================================
# FINAL CONCLUSION
# ================================

print("\n" + "="*60)
print("FINAL MODEL CONCLUSION")
print("="*60)

accuracy_percentage = 100 - (val_mae / y_val.mean() * 100)

print(f"ğŸ“Š Model Performance:")
print(f"   - Validation MAE: {val_mae:.4f}")
print(f"   - Model Accuracy: {accuracy_percentage:.2f}%")
print(f"   - RÂ² Score: {val_r2:.4f}")

if val_r2 > 0.8:
    performance = "Excellent"
elif val_r2 > 0.6:
    performance = "Good"
elif val_r2 > 0.4:
    performance = "Fair"
else:
    performance = "Poor"

print(f"   - Performance Rating: {performance}")

print(f"\nğŸ” Model Analysis:")
if mae_difference < 1.0:
    print("   - Model generalizes well to unseen data")
else:
    print("   - Model may have slight overfitting issues")

print(f"\nğŸ“ Output Files:")
print(f"   - submission.csv (ready for Kaggle submission)")
print(f"   - model_evaluation.png (performance graphs)")

print("\nğŸ¯ Recommendations for improvement:")
print("   - Try ensemble methods with traditional ML models")
print("   - Experiment with different architectures")
print("   - Add more feature engineering")
print("   - Tune hyperparameters further")